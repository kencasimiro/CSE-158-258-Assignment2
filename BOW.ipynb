{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5664bbdb-ec6f-4930-8e56-ca1a6521ca42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK stopwords...\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kcasimiro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Cleaning data...\n",
      "Removing stopwords...\n",
      "Data cleaning completed.\n",
      "Splitting dataset into train, validation, and test sets...\n",
      "Dataset split successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download stopwords\n",
    "print(\"Downloading NLTK stopwords...\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the JSONL file into a DataFrame\n",
    "print(\"Loading dataset...\")\n",
    "file_path = \"Cell_Phones_and_Accessories_5.json\"\n",
    "data = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Data Cleaning\n",
    "print(\"Cleaning data...\")\n",
    "df.dropna(subset=['reviewText', 'overall'], inplace=True)  # Remove rows with missing values\n",
    "df['reviewText'] = df['reviewText'].str.lower()  # Convert text to lowercase\n",
    "df['reviewText'] = df['reviewText'].str.replace(r'[^\\w\\s]', '', regex=True)  # Remove punctuation\n",
    "\n",
    "# Remove stopwords\n",
    "print(\"Removing stopwords...\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['filteredText'] = df['reviewText'].apply(\n",
    "    lambda x: \" \".join(word for word in x.split() if word not in stop_words)\n",
    ")\n",
    "print(\"Data cleaning completed.\")\n",
    "\n",
    "# Dataset Splitting\n",
    "print(\"Splitting dataset into train, validation, and test sets...\")\n",
    "train, temp = train_test_split(df, test_size=0.2, random_state=42)\n",
    "validation, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "print(\"Dataset split successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce90609f-9c0c-4e87-8250-ecb3eb9796b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing text using Bag of Words...\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words Vectorization\n",
    "print(\"Vectorizing text using Bag of Words...\")\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X_train = vectorizer.fit_transform(train['filteredText']).toarray()\n",
    "X_validation = vectorizer.transform(validation['filteredText']).toarray()\n",
    "X_test = vectorizer.transform(test['filteredText']).toarray()\n",
    "print(\"Vectorization completed.\")\n",
    "\n",
    "# Target Variable\n",
    "y_train = train['overall']\n",
    "y_validation = validation['overall']\n",
    "y_test = test['overall']\n",
    "\n",
    "# Parameter Grid for Logistic Regression\n",
    "param_grid = {'C': [0.001, 0.0001, 0.00001, 0.01, 0.1, 1, 10]}\n",
    "logistic = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')\n",
    "\n",
    "# Grid Search for Optimal C\n",
    "print(\"Performing Grid Search for optimal hyperparameter C...\")\n",
    "grid_search = GridSearchCV(logistic, param_grid, cv=5, scoring='accuracy', verbose=1, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Grid Search completed.\")\n",
    "\n",
    "# Output validation accuracies for each C value\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(\"Validation Accuracies for Different C Values:\")\n",
    "for c, mean_val_score in zip(results['param_C'], results['mean_test_score']):\n",
    "    print(f\"C: {c} -> Validation Accuracy: {mean_val_score:.4f}\")\n",
    "\n",
    "# Best Model Evaluation\n",
    "best_c = grid_search.best_params_['C']\n",
    "print(f\"\\nOptimal C value: {best_c}\")\n",
    "\n",
    "print(\"Evaluating the best model on the test set...\")\n",
    "best_model = grid_search.best_estimator_\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Test Accuracy and Classification Report\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"\\nTest Accuracy with optimal C: {test_accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Save Vectorizer and Model (Optional)\n",
    "print(\"Saving the vectorizer and model...\")\n",
    "import pickle\n",
    "with open('bow_vectorizer.pkl', 'wb') as vec_file:\n",
    "    pickle.dump(vectorizer, vec_file)\n",
    "with open('logistic_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_model, model_file)\n",
    "print(\"Vectorizer and model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7ad627-00a6-4768-a088-3e3aa03fed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK stopwords...\n",
      "Loading dataset...\n",
      "Dataset loaded successfully with 10000 records.\n",
      "Cleaning data...\n",
      "Removing stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kcasimiro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning completed.\n",
      "Splitting dataset into train, validation, and test sets...\n",
      "Dataset split successfully.\n",
      "Vectorizing text using Bag of Words...\n",
      "Vectorization completed.\n",
      "Performing Grid Search for optimal hyperparameter C...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Grid Search completed.\n",
      "Validation Accuracies for Different C Values:\n",
      "C: 0.0001 -> Validation Accuracy: 0.5169\n",
      "C: 0.001 -> Validation Accuracy: 0.5205\n",
      "C: 0.01 -> Validation Accuracy: 0.5515\n",
      "C: 0.1 -> Validation Accuracy: 0.5827\n",
      "C: 1 -> Validation Accuracy: 0.5650\n",
      "C: 10 -> Validation Accuracy: 0.5326\n",
      "C: 100 -> Validation Accuracy: 0.5134\n",
      "C: 1000 -> Validation Accuracy: 0.5004\n",
      "\n",
      "Optimal C value: 0.1\n",
      "Evaluating the best model on the test set...\n",
      "\n",
      "Test Accuracy with optimal C: 0.58\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.55      0.43      0.49        97\n",
      "         2.0       0.33      0.14      0.20        71\n",
      "         3.0       0.42      0.14      0.20       111\n",
      "         4.0       0.40      0.25      0.31       204\n",
      "         5.0       0.64      0.90      0.75       517\n",
      "\n",
      "    accuracy                           0.58      1000\n",
      "   macro avg       0.47      0.37      0.39      1000\n",
      "weighted avg       0.53      0.58      0.53      1000\n",
      "\n",
      "Saving the vectorizer and model...\n",
      "Vectorizer and model saved successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ................................................C=1; total time=   0.6s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ................................................C=1; total time=   0.6s\n",
      "[CV] END ...............................................C=10; total time=   1.1s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ...............................................C=10; total time=   1.1s\n",
      "[CV] END ...............................................C=10; total time=   1.2s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ...............................................C=10; total time=   1.1s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ................................................C=1; total time=   0.7s\n",
      "[CV] END ...............................................C=10; total time=   1.0s\n",
      "[CV] END ...........................................C=0.0001; total time=   0.1s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ................................................C=1; total time=   0.6s\n",
      "[CV] END ................................................C=1; total time=   0.6s\n",
      "[CV] END ..............................................C=100; total time=   1.9s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ................................................C=1; total time=   0.6s\n",
      "[CV] END ...............................................C=10; total time=   1.0s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ...............................................C=10; total time=   1.1s\n",
      "[CV] END ...........................................C=0.0001; total time=   0.1s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ................................................C=1; total time=   0.7s\n",
      "[CV] END ................................................C=1; total time=   0.6s\n",
      "[CV] END .............................................C=1000; total time=   2.2s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ................................................C=1; total time=   0.6s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ...............................................C=10; total time=   1.1s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ..............................................C=100; total time=   1.8s\n",
      "[CV] END ..............................................C=100; total time=   1.7s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ................................................C=1; total time=   0.6s\n",
      "[CV] END ...............................................C=10; total time=   1.1s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ...............................................C=10; total time=   1.1s\n",
      "[CV] END ...........................................C=0.0001; total time=   0.1s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ................................................C=1; total time=   0.7s\n",
      "[CV] END ...............................................C=10; total time=   1.1s\n",
      "[CV] END .............................................C=1000; total time=   2.2s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ...............................................C=10; total time=   1.1s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ................................................C=1; total time=   0.7s\n",
      "[CV] END ...........................................C=0.0001; total time=   0.1s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ..............................................C=100; total time=   1.6s\n",
      "[CV] END .............................................C=1000; total time=   1.9s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END .............................................C=0.01; total time=   0.1s\n",
      "[CV] END ................................................C=1; total time=   0.7s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ................................................C=1; total time=   0.6s\n",
      "[CV] END ...........................................C=0.0001; total time=   0.1s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ..............................................C=100; total time=   1.7s\n",
      "[CV] END .............................................C=1000; total time=   2.0s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ...............................................C=10; total time=   1.1s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END ..............................................C=0.1; total time=   0.3s\n",
      "[CV] END ................................................C=1; total time=   0.7s\n",
      "[CV] END ............................................C=0.001; total time=   0.1s\n",
      "[CV] END .............................................C=0.01; total time=   0.2s\n",
      "[CV] END ...............................................C=10; total time=   1.1s\n",
      "[CV] END ...............................................C=10; total time=   1.1s\n",
      "[CV] END .............................................C=1000; total time=   2.2s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "import gc  # Garbage collection for memory management\n",
    "\n",
    "# Download stopwords\n",
    "print(\"Downloading NLTK stopwords...\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load a subset of the JSONL file into a DataFrame\n",
    "print(\"Loading dataset...\")\n",
    "file_path = \"Cell_Phones_and_Accessories_5.json\"\n",
    "data = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 10000:  # Limit to 10,000 reviews for debugging\n",
    "            break\n",
    "        data.append(json.loads(line))\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Dataset loaded successfully with {len(df)} records.\")\n",
    "\n",
    "# Data Cleaning\n",
    "print(\"Cleaning data...\")\n",
    "df.dropna(subset=['reviewText', 'overall'], inplace=True)  # Remove rows with missing values\n",
    "df['reviewText'] = df['reviewText'].str.lower()  # Convert text to lowercase\n",
    "df['reviewText'] = df['reviewText'].str.replace(r'[^\\w\\s]', '', regex=True)  # Remove punctuation\n",
    "\n",
    "# Remove stopwords\n",
    "print(\"Removing stopwords...\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['filteredText'] = df['reviewText'].apply(\n",
    "    lambda x: \" \".join(word for word in x.split() if word not in stop_words)\n",
    ")\n",
    "print(\"Data cleaning completed.\")\n",
    "\n",
    "# Dataset Splitting\n",
    "print(\"Splitting dataset into train, validation, and test sets...\")\n",
    "train, temp = train_test_split(df, test_size=0.2, random_state=42, stratify=df['overall'])\n",
    "validation, test = train_test_split(temp, test_size=0.5, random_state=42, stratify=temp['overall'])\n",
    "print(\"Dataset split successfully.\")\n",
    "\n",
    "# Bag of Words Vectorization\n",
    "print(\"Vectorizing text using Bag of Words...\")\n",
    "vectorizer = CountVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "X_train = vectorizer.fit_transform(train['filteredText'])  # Sparse matrix\n",
    "X_validation = vectorizer.transform(validation['filteredText'])  # Sparse matrix\n",
    "X_test = vectorizer.transform(test['filteredText'])  # Sparse matrix\n",
    "print(\"Vectorization completed.\")\n",
    "\n",
    "# Target Variable\n",
    "y_train = train['overall']\n",
    "y_validation = validation['overall']\n",
    "y_test = test['overall']\n",
    "\n",
    "# Parameter Grid for Logistic Regression\n",
    "param_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "logistic = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')\n",
    "\n",
    "# Grid Search for Optimal C\n",
    "print(\"Performing Grid Search for optimal hyperparameter C...\")\n",
    "grid_search = GridSearchCV(\n",
    "    logistic, param_grid, cv=5, scoring='accuracy', verbose=2, n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Grid Search completed.\")\n",
    "\n",
    "# Output validation accuracies for each C value\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(\"Validation Accuracies for Different C Values:\")\n",
    "for c, mean_val_score in zip(results['param_C'], results['mean_test_score']):\n",
    "    print(f\"C: {c} -> Validation Accuracy: {mean_val_score:.4f}\")\n",
    "\n",
    "# Best Model Evaluation\n",
    "best_c = grid_search.best_params_['C']\n",
    "print(f\"\\nOptimal C value: {best_c}\")\n",
    "\n",
    "print(\"Evaluating the best model on the test set...\")\n",
    "best_model = grid_search.best_estimator_\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Test Accuracy and Classification Report\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"\\nTest Accuracy with optimal C: {test_accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Save Vectorizer and Model\n",
    "print(\"Saving the vectorizer and model...\")\n",
    "with open('bow_vectorizer.pkl', 'wb') as vec_file:\n",
    "    pickle.dump(vectorizer, vec_file)\n",
    "with open('logistic_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_model, model_file)\n",
    "print(\"Vectorizer and model saved successfully.\")\n",
    "\n",
    "# Cleanup\n",
    "del X_train, X_validation, X_test\n",
    "gc.collect()  # Free memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db05e7f-3fdb-4a1b-a94d-816bdc0f08f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK stopwords...\n",
      "Loading dataset...\n",
      "Dataset loaded successfully with 10000 records.\n",
      "Cleaning data...\n",
      "Removing stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kcasimiro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning completed.\n",
      "Splitting dataset into train, validation, and test sets...\n",
      "Dataset split successfully.\n",
      "Vectorizing text using TF-IDF...\n",
      "Vectorization completed.\n",
      "Selecting top features using Chi-Square Test...\n",
      "Feature selection completed.\n",
      "Performing Grid Search for optimal hyperparameter C...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Grid Search completed.\n",
      "Validation Accuracies for Different C Values:\n",
      "C: 0.001 -> Validation Accuracy: 0.5169\n",
      "C: 0.01 -> Validation Accuracy: 0.5169\n",
      "C: 0.1 -> Validation Accuracy: 0.5167\n",
      "C: 1 -> Validation Accuracy: 0.5835\n",
      "C: 10 -> Validation Accuracy: 0.6109\n",
      "\n",
      "Optimal C value: 10\n",
      "Evaluating the best model on the test set...\n",
      "\n",
      "Test Accuracy with optimal C: 0.58\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.55      0.43      0.48        97\n",
      "         2.0       0.23      0.08      0.12        71\n",
      "         3.0       0.38      0.18      0.25       111\n",
      "         4.0       0.39      0.35      0.37       204\n",
      "         5.0       0.67      0.85      0.75       517\n",
      "\n",
      "    accuracy                           0.58      1000\n",
      "   macro avg       0.44      0.38      0.39      1000\n",
      "weighted avg       0.53      0.58      0.54      1000\n",
      "\n",
      "Saving the vectorizer, selector, and model...\n",
      "Vectorizer, selector, and model saved successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# Download stopwords\n",
    "print(\"Downloading NLTK stopwords...\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load a subset of the dataset for optimization\n",
    "print(\"Loading dataset...\")\n",
    "file_path = \"Cell_Phones_and_Accessories_5.json\"\n",
    "data = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 10000:  # Limit to first 10,000 reviews for optimization\n",
    "            break\n",
    "        data.append(json.loads(line))\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Dataset loaded successfully with {len(df)} records.\")\n",
    "\n",
    "# Data Cleaning\n",
    "print(\"Cleaning data...\")\n",
    "df.dropna(subset=['reviewText', 'overall'], inplace=True)  # Remove rows with missing values\n",
    "df['reviewText'] = df['reviewText'].str.lower()  # Convert text to lowercase\n",
    "df['reviewText'] = df['reviewText'].str.replace(r'[^\\w\\s]', '', regex=True)  # Remove punctuation\n",
    "\n",
    "# Remove stopwords\n",
    "print(\"Removing stopwords...\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['filteredText'] = df['reviewText'].apply(\n",
    "    lambda x: \" \".join(word for word in x.split() if word not in stop_words)\n",
    ")\n",
    "print(\"Data cleaning completed.\")\n",
    "\n",
    "# Dataset Splitting\n",
    "print(\"Splitting dataset into train, validation, and test sets...\")\n",
    "train, temp = train_test_split(df, test_size=0.2, random_state=42, stratify=df['overall'])\n",
    "validation, test = train_test_split(temp, test_size=0.5, random_state=42, stratify=temp['overall'])\n",
    "print(\"Dataset split successfully.\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "print(\"Vectorizing text using TF-IDF...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,  # Top 10,000 words\n",
    "    min_df=5,  # Ignore words in fewer than 5 documents\n",
    "    max_df=0.9,  # Ignore very common words in 90%+ documents\n",
    "    ngram_range=(1, 2)  # Unigrams and bigrams\n",
    ")\n",
    "X_train = vectorizer.fit_transform(train['filteredText'])  # Sparse matrix\n",
    "X_validation = vectorizer.transform(validation['filteredText'])  # Sparse matrix\n",
    "X_test = vectorizer.transform(test['filteredText'])  # Sparse matrix\n",
    "print(\"Vectorization completed.\")\n",
    "\n",
    "# Feature Selection\n",
    "print(\"Selecting top features using Chi-Square Test...\")\n",
    "selector = SelectKBest(chi2, k=5000)  # Select top 5,000 features\n",
    "X_train_reduced = selector.fit_transform(X_train, train['overall'])\n",
    "X_validation_reduced = selector.transform(X_validation)\n",
    "X_test_reduced = selector.transform(X_test)\n",
    "print(\"Feature selection completed.\")\n",
    "\n",
    "# Target Variable\n",
    "y_train = train['overall']\n",
    "y_validation = validation['overall']\n",
    "y_test = test['overall']\n",
    "\n",
    "# Parameter Grid for Logistic Regression\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "logistic = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')\n",
    "\n",
    "# Grid Search for Optimal C\n",
    "print(\"Performing Grid Search for optimal hyperparameter C...\")\n",
    "grid_search = GridSearchCV(\n",
    "    logistic, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_reduced, y_train)\n",
    "print(\"Grid Search completed.\")\n",
    "\n",
    "# Output validation accuracies for each C value\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(\"Validation Accuracies for Different C Values:\")\n",
    "for c, mean_val_score in zip(results['param_C'], results['mean_test_score']):\n",
    "    print(f\"C: {c} -> Validation Accuracy: {mean_val_score:.4f}\")\n",
    "\n",
    "# Best Model Evaluation\n",
    "best_c = grid_search.best_params_['C']\n",
    "print(f\"\\nOptimal C value: {best_c}\")\n",
    "\n",
    "print(\"Evaluating the best model on the test set...\")\n",
    "best_model = grid_search.best_estimator_\n",
    "y_test_pred = best_model.predict(X_test_reduced)\n",
    "\n",
    "# Test Accuracy and Classification Report\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"\\nTest Accuracy with optimal C: {test_accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Save Vectorizer, Selector, and Model\n",
    "print(\"Saving the vectorizer, selector, and model...\")\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as vec_file:\n",
    "    pickle.dump(vectorizer, vec_file)\n",
    "with open('feature_selector.pkl', 'wb') as sel_file:\n",
    "    pickle.dump(selector, sel_file)\n",
    "with open('logistic_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_model, model_file)\n",
    "print(\"Vectorizer, selector, and model saved successfully.\")\n",
    "\n",
    "# Cleanup\n",
    "del X_train, X_train_reduced, X_validation, X_validation_reduced, X_test, X_test_reduced\n",
    "gc.collect()  # Free memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ebfe7e-9b18-4e01-bbd8-3fa835c98f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
